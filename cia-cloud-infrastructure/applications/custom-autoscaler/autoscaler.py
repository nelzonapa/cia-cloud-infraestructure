#!/usr/bin/env python3
"""
Controlador de Auto-Scaling Personalizado para aplicaci√≥n IoT.
Monitorea m√©tricas personalizadas y ajusta el n√∫mero de r√©plicas.
"""
import time
import logging
import requests
import json
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CustomAutoscaler:
    def __init__(self, app_name, namespace="default"):
        self.app_name = app_name
        self.namespace = namespace
        self.min_replicas = 1
        self.max_replicas = 5
        self.target_queue_size = 3  # Objetivo: mantener cola alrededor de 3 elementos
        
        # Configurar cliente Kubernetes
        try:
            config.load_incluster_config()  # Dentro del cluster
        except:
            config.load_kube_config()       # Para desarrollo local
        
        self.apps_v1 = client.AppsV1Api()

    def get_app_metrics(self):
        """Obtiene m√©tricas de la aplicaci√≥n IoT"""
        try:
            # Usar el servicio interno para obtener m√©tricas
            service_url = f"http://iot-processor-service.default.svc.cluster.local"
            
            # Obtener m√©tricas de Prometheus
            metrics_response = requests.get(f"{service_url}/metrics", timeout=5)
            metrics_data = {}
            
            if metrics_response.status_code == 200:
                for line in metrics_response.text.split('\n'):
                    if line.startswith('processing_queue_size'):
                        value = float(line.split()[1])
                        metrics_data['queue_size'] = value
                    elif line.startswith('active_requests'):
                        value = float(line.split()[1])
                        metrics_data['active_requests'] = value
            
            # Obtener estado actual de la aplicaci√≥n
            status_response = requests.get(f"{service_url}/sensor-data", timeout=5)
            if status_response.status_code == 200:
                status_data = status_response.json()
                metrics_data['total_processed'] = status_data.get('total_processed', 0)
                metrics_data['current_queue'] = status_data.get('queue_size', 0)
            
            return metrics_data
            
        except Exception as e:
            logger.error(f"Error obteniendo m√©tricas: {e}")
            return {'queue_size': 0, 'active_requests': 0, 'current_queue': 0}

    def get_current_replicas(self):
        """Obtiene el n√∫mero actual de r√©plicas del deployment"""
        try:
            deployment = self.apps_v1.read_namespaced_deployment(
                name=self.app_name,
                namespace=self.namespace
            )
            return deployment.spec.replicas
        except ApiException as e:
            logger.error(f"Error obteniendo r√©plicas: {e}")
            return None

    def scale_deployment(self, new_replicas):
        """Escala el deployment al n√∫mero especificado de r√©plicas"""
        try:
            if new_replicas < self.min_replicas:
                new_replicas = self.min_replicas
            elif new_replicas > self.max_replicas:
                new_replicas = self.max_replicas
            
            # Obtener deployment actual
            deployment = self.apps_v1.read_namespaced_deployment(
                name=self.app_name,
                namespace=self.namespace
            )
            
            current_replicas = deployment.spec.replicas
            
            if current_replicas != new_replicas:
                # Actualizar n√∫mero de r√©plicas
                deployment.spec.replicas = new_replicas
                
                # Aplicar cambios
                self.apps_v1.patch_namespaced_deployment(
                    name=self.app_name,
                    namespace=self.namespace,
                    body=deployment
                )
                
                logger.info(f"üöÄ Auto-scaling: {current_replicas} ‚Üí {new_replicas} r√©plicas")
                return True
            else:
                logger.info(f"‚úÖ R√©plicas estables en {new_replicas}")
                return False
                
        except ApiException as e:
            logger.error(f"Error escalando deployment: {e}")
            return False

    def calculate_desired_replicas(self, metrics):
        """Calcula el n√∫mero deseado de r√©plicas basado en las m√©tricas"""
        current_replicas = self.get_current_replicas()
        if current_replicas is None:
            return self.min_replicas
        
        queue_size = metrics.get('queue_size', 0)
        current_queue = metrics.get('current_queue', 0)
        active_requests = metrics.get('active_requests', 0)
        
        logger.info(f"üìä M√©tricas - Cola: {queue_size}, Requests activas: {active_requests}")
        
        # L√≥gica de escalado personalizada
        desired_replicas = current_replicas
        
        # Escalar basado en el tama√±o de la cola
        if queue_size > self.target_queue_size * 2:  # Cola muy grande
            desired_replicas = min(self.max_replicas, current_replicas + 2)
            logger.info(f"üìà Cola grande ({queue_size}), escalando agresivamente")
            
        elif queue_size > self.target_queue_size:  # Cola por encima del objetivo
            desired_replicas = min(self.max_replicas, current_replicas + 1)
            logger.info(f"üìà Cola creciendo ({queue_size}), escalando")
            
        elif queue_size < self.target_queue_size / 2 and current_replicas > self.min_replicas:  # Cola muy peque√±a
            desired_replicas = max(self.min_replicas, current_replicas - 1)
            logger.info(f"üìâ Cola peque√±a ({queue_size}), reduciendo")
        
        # Tambi√©n considerar requests activas
        if active_requests > current_replicas * 3 and desired_replicas <= current_replicas:
            desired_replicas = min(self.max_replicas, current_replicas + 1)
            logger.info(f"üî• Muchas requests activas ({active_requests}), escalando")
        
        return desired_replicas

    def run(self):
        """Bucle principal del autoscaler"""
        logger.info(f"üéØ Iniciando autoscaler personalizado para {self.app_name}")
        logger.info(f"üìè Configuraci√≥n: Min={self.min_replicas}, Max={self.max_replicas}, TargetQueue={self.target_queue_size}")
        
        while True:
            try:
                # Obtener m√©tricas
                metrics = self.get_app_metrics()
                
                # Calcular r√©plicas deseadas
                desired_replicas = self.calculate_desired_replicas(metrics)
                
                # Aplicar escalado si es necesario
                self.scale_deployment(desired_replicas)
                
                # Esperar antes de la siguiente iteraci√≥n
                time.sleep(30)  # Revisar cada 30 segundos
                
            except Exception as e:
                logger.error(f"‚ùå Error en bucle principal: {e}")
                time.sleep(60)  # Esperar m√°s en caso de error

if __name__ == "__main__":
    autoscaler = CustomAutoscaler(app_name="iot-processor")
    autoscaler.run()
